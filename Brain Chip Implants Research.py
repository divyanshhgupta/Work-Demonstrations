# -*- coding: utf-8 -*-
"""AI project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17cMmKjfQpUt3e0XPzVxJUOgKoQ_TgREF
"""

from google.colab import drive
drive.mount('/content/drive')
import pandas as pd
import numpy as np
from scipy import stats
from openpyxl import load_workbook

file_path = '/content/drive/My Drive/Colab Notebooks/AI-Human-PewData.xlsx'
data = pd.read_excel(file_path)

# Step 1: Identify Missing Values
missing_values = data.isnull().sum()
missing_values_summary = missing_values[missing_values > 0]
print("Missing values per column:\n", missing_values_summary)

# Step 2: Handle Missing Values
# Fill missing values in TECH1_W99 and SC1_W99 with their respective modes
if 'TECH1_W99' in data.columns:
    data['TECH1_W99'].fillna(data['TECH1_W99'].mode()[0], inplace=True)
if 'SC1_W99' in data.columns:
    data['SC1_W99'].fillna(data['SC1_W99'].mode()[0], inplace=True)

# Drop columns with excessive missing values that are not critical for analysis
columns_to_drop = ['EXCITEOE_W99_OE1', 'EXCITEOE_W99_OE2', 'EXCITEOE_W99_OE3']
data.drop(columns=columns_to_drop, inplace=True)

# Fill missing values in demographic columns with mode
demographic_columns = ['F_PARTYLN_FINAL', 'F_REG', 'F_INTFREQ']
for col in demographic_columns:
    data[col].fillna(data[col].mode()[0], inplace=True)

# Step 3: Remove Duplicate Records
data_cleaned = data.drop_duplicates().copy()

# Step 4: Convert Data Types
if 'INTERVIEW_START_W99' in data_cleaned.columns:
    data_cleaned['INTERVIEW_START_W99'] = pd.to_datetime(data_cleaned['INTERVIEW_START_W99'])
if 'INTERVIEW_END_W99' in data_cleaned.columns:
    data_cleaned['INTERVIEW_END_W99'] = pd.to_datetime(data_cleaned['INTERVIEW_END_W99'])

# Step 5: Standardize Categorical Variables
if 'F_VOLSUM' in data_cleaned.columns:
    data_cleaned['F_VOLSUM'] = data_cleaned['F_VOLSUM'].str.lower()
    data_cleaned['F_VOLSUM'].replace({'yes': 'Yes', 'no': 'No'}, inplace=True)

# Step 6.1: Handle Outliers(Not good to use it here. Check the notes to see the explanation)
# For demonstration, handle outliers in WEIGHT_W99
#Q1 = data_cleaned['WEIGHT_W99'].quantile(0.25)
#Q3 = data_cleaned['WEIGHT_W99'].quantile(0.75)
#IQR = Q3 - Q1
#lower_bound = Q1 - 1.5 * IQR
#upper_bound = Q3 + 1.5 * IQR
#data_cleaned = data_cleaned[(data_cleaned['WEIGHT_W99'] >= lower_bound) & (data_cleaned['WEIGHT_W99'] <= upper_bound)]

# Step 6.2: Handle Outliers using Z-score method
if 'WEIGHT_W99' in data_cleaned.columns:
    data_cleaned['z_score'] = stats.zscore(data_cleaned['WEIGHT_W99'])
    data_cleaned = data_cleaned[(data_cleaned['z_score'] >= -3) & (data_cleaned['z_score'] <= 3)].copy()
    data_cleaned.drop(columns=['z_score'], inplace=True)

# Display cleaned dataset summary
data_cleaned.info()
data_cleaned.head()

from google.colab import files

# Save the file locally in the Colab environment
cleaned_file_path = '/content/AI-Human-PewData_Cleaned.xlsx'
data_cleaned.to_excel(cleaned_file_path, index=False)

# Download the file directly
files.download(cleaned_file_path)

#Second round of cleaning after knowing we wanted to study each age group's willingness to adopt brain chip implants(Performed in a different environoment)
# Step 7: Create the age groups
 # Map the existing age categories to the new age groups, handling 'Refused'
 #def map_age_group(age_category):
     #if age_category in ['18-29', '30-49', '50-64', '65+']:
             #return age_category
     #else:
             #return 'Unknown'
#data['Age Group'] = data['F_AGECAT'].apply(map_age_group)

# Step 8: we want to retain Form 1 responses and handle NaNs from BCHIP columns. The purpuse is to study people's wilingness to adopt brain chip implants.
# imputed_data = data.copy()
# for col in bchip_columns:
#    imputed_data[col].fillna('No response', inplace=True)

# Re-clean the data to retain Form 1 responses and handle NaNs differently
# Instead of removing rows with NaNs in BCHIP columns, we will impute NaNs with a placeholder value

# Impute NaN values in BCHIP columns with 'No response'
# imputed_data = data.copy()
# for col in bchip_columns:
#     imputed_data[col].fillna('No response', inplace=True)

# Verify if all columns are retained
# all_columns_present = all(col in imputed_data.columns for col in [
#     'SMALG3_W99', 'SMALG4_a_W99', 'SMALG4_b_W99', 'SMALG4_c_W99', 'SMALG4_d_W99',
#     'SMALG5_W99', 'SMALG6_W99', 'SMALG7_W99', 'SMALG8_W99', 'SMALG9_a_W99', 'SMALG9_b_W99',
#     'SMALG9_c_W99', 'SMALG10_a_W99', 'SMALG10_b_W99', 'SMALG10_c_W99', 'SMALG11_W99', 'SMALG12_W99',
#     'SMALG13_a_W99', 'SMALG13_b_W99', 'SMALG13_c_W99', 'SMALG13_d_W99', 'FACEREC1_W99', 'FACEREC2_W99',
#     'FACEREC3_a_W99', 'FACEREC3_b_W99', 'FACEREC3_c_W99', 'FACEREC3_d_W99', 'FACEREC3_e_W99', 'FACEREC3_f_W99',
#     'FACEREC4_W99', 'FACEREC5_W99', 'FACEREC6_a_W99', 'FACEREC6_b_W99', 'FACEREC6_c_W99', 'FACEREC6_d_W99',
#     'FACEREC7_W99', 'FACEREC8_a_W99', 'FACEREC8_b_W99', 'FACEREC8_c_W99', 'FACEREC9_W99', 'FACEREC10_W99',
#     'FACEREC11_a_W99', 'FACEREC11_b_W99', 'FACEREC11_c_W99', 'FACEREC12_a_W99', 'FACEREC12_b_W99', 'FACEREC12_c_W99',
#     'FACEREC12_d_W99', 'DCARS1_W99', 'DCARS2_W99', 'DCARS3_W99', 'DCARS4_a_W99', 'DCARS4_b_W99', 'DCARS4_c_W99',
#     'DCARS4_d_W99', 'DCARS5_W99', 'DCARS6_W99', 'DCARS7_W99', 'DCARS8_a_W99', 'DCARS8_b_W99', 'DCARS8_c_W99',
#     'DCARS8_d_W99', 'DCARS9_W99', 'DCARS10_W99', 'DCARS11_a_W99', 'DCARS11_b_W99', 'DCARS11_c_W99', 'DCARS11_d_W99',
#     'DCARS12_W99', 'DCARS13_a_W99', 'DCARS13_b_W99', 'DCARS13_c_W99', 'DCARS13_d_W99'
# ])

# Save the re-cleaned data to a new Excel file
# recleaned_file_path = '/mnt/data/Recleaned_PewData_With_AgeGroups.xlsx'
# imputed_data.to_excel(recleaned_file_path, index=False)

# all_columns_present, recleaned_file_path

#Documentation

# Summary of Data Cleaning Steps and Justifications(First round)

# 1. Identify Missing Values:
# Identified columns with missing values to determine the extent and significance.
# Justification: Understanding which columns have missing values helps in deciding the appropriate strategy to handle them.

# 2. Handle Missing Values:
# Imputed missing values in key columns like TECH1_W99 and SC1_W99 with their respective modes.
# Dropped columns with excessive missing values that are not critical for analysis (EXCITEOE_W99_OE1, EXCITEOE_W99_OE2, EXCITEOE_W99_OE3).
# Filled missing values in demographic columns (F_PARTYLN_FINAL, F_REG, F_INTFREQ) with the mode.
# Justification: Imputing and dropping columns ensures that the dataset remains usable and complete without losing critical information.

# 3. Remove Duplicate Records:
# Removed any duplicate records to maintain data integrity.
# Justification: Ensures that each response is unique and accurate.

# 4. Convert Data Types:
# Converted INTERVIEW_START_W99 and INTERVIEW_END_W99 to datetime format.
# Justification: Correct data types facilitate accurate analysis, especially for time-based calculations.

# 5. Standardize Categorical Variables:
# Standardized F_VOLSUM values to ensure consistency (yes to Yes and no to No).
# Justification: Consistency in categorical values is crucial for accurate analysis and visualization.

# 6.1 Handle Outliers:(This IQR method impacted the data too much)
# Identified and handled outliers in WEIGHT_W99 by using the interquartile range (IQR) method.
# Justification: Handling outliers prevents skewed results and ensures a more accurate representation of the data.
# Note:# Outliers in the WEIGHT_W99 column were initially addressed using the Z-score method.
# The handling of outliers in the WEIGHT_W99 column indicates some discrepancies.
# This suggests that the outlier removal process may have impacted the integrity of the dataset,
# potentially requiring a review of the specific thresholds and methodology used.

# 6.2 Handling Outliers (Z-score method):
# Outliers in the WEIGHT_W99 column were addressed using the Z-score method, ensuring that only values within three standard deviations were retained.
# The Z-score method has appropriately handled the outliers in the WEIGHT_W99 column, resulting in a cleaner and more accurate dataset for analysis.
# The removal of outliers reduces the impact of extreme values, providing more reliable insights into the central tendencies and variability within the data.
# Step 6.3: Create age groups
# The age groups (18-29, 30-49, 50-64, 65+) were created based on common demographic categories used in research to analyze trends and attitudes across different stages of life.
# Outlier Detection Summary

# The outlier detection results show discrepancies between the IQR and Z-score methods:

# IQR Method Outliers: 237
# Z-Score Method Outliers: 64
# Lower Bound (IQR): -0.309
# Upper Bound (IQR): 1.550

# Observations:
# IQR Method: Identified a larger number of outliers (237). This method may be more sensitive to the distribution of the data.
# Z-Score Method: Identified fewer outliers (64). This method may provide a more conservative estimate of outliers based on standard deviations from the mean.

#Cleaned Dataset Summary(6.2):
#Number of Entries: 9,972
#Number of Columns: 212

# Conclusion(6.2)
# The data cleaning steps we implemented align well with the typical practices employed by Pew Research Center.
# The publisher likely performed extensive preprocessing to ensure data accuracy and consistency, including handling missing values,
# standardizing responses, validating data, and categorizing open-ended responses.
# The cleaning steps did not inadvertently remove or alter data that is critical to the survey design.
# The columns with missing values due to the survey design have been handled appropriately,
# with critical columns being filled with the mode and excessive missing columns being dropped.

# Determining Critical Data in Survey Design

# When working with survey data, certain columns and values are considered critical to the integrity and validity of the survey design.
# Here are the factors and considerations used to determine which data is critical:

# Core Survey Questions:
# These are the primary questions that address the main objectives of the survey.
# In the Pew Research survey, questions like TECH1_W99 (impact of technology on society) and SC1_W99 (impact of science on society)
# are core questions directly related to the research topic.

# Demographic Information:
# Demographic variables such as age, gender, income, education, and political affiliation are essential for analyzing and segmenting responses.
# They provide context and allow for subgroup analysis.
# In this survey, columns like F_PARTYLN_FINAL (political party affiliation), F_REG (voter registration status), and F_INTFREQ (internet usage frequency)
# are critical demographic variables.

# Survey Design and Structure:
# Some questions are designed to be asked only to specific groups or forms.
# For example, FORM_W99 indicates whether the respondent answered Form 1 or Form 2, and missing values in such cases are expected and should be preserved.

# Response Options and Follow-up Questions:
# Questions that lead to follow-up questions based on initial responses are critical.
# For example, if a respondent answers they are excited about AI, they may be asked to elaborate on the reason (open-ended responses like EXCITEOE_W99_OE1).

# Weighting Variables:
# Weight variables like WEIGHT_W99 are critical for ensuring that the survey results are representative of the broader population.
# Proper handling of these variables is essential for accurate analysis.

# Examples from the Cleaning Process:

# Filling Missing Values:
# For critical columns like TECH1_W99 and SC1_W99, missing values were filled with the mode to retain as much data as possible while ensuring the responses remain valid.

# Dropping Non-Critical Columns:
# Columns with excessive missing values that were not central to the analysis or had open-ended responses that are less critical for statistical analysis were dropped.
# Examples include EXCITEOE_W99_OE1, EXCITEOE_W99_OE2, and EXCITEOE_W99_OE3.

# Handling Outliers:
# The WEIGHT_W99 column was cleaned by removing extreme outliers using the Z-score method to ensure accurate representation and avoid skewing the results.

# Preserving Survey Design-Related Missing Values:
# Columns with missing values due to the survey design (e.g., questions not asked to all respondents) were preserved to maintain the integrity of the survey structure.